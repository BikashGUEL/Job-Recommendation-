{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GTA University dataset is imported.\n",
    "For student privacy random student id is generated with format of stu_xxxx.\n",
    "From (q1-q37)survey questions. Follow columns name are changed and new dataset name 'student_dataset2.csv' is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random \n",
    "\n",
    "student_dataset = pd.read_csv('student_dataset.csv')\n",
    "total_rows = len(student_dataset)\n",
    "student_ids = ['stu_' + str(random.randint(1000, 9999)) for _ in range(total_rows)]\n",
    "student_dataset.insert(0,'student_id',student_ids)\n",
    "# student_dataset= student_dataset[['student_id','q1','q2','q7','q8','q11','q20','q30','q31']]\n",
    "student_dataset.rename(columns={'q1':'course_name','q2':'enrolled_course','q7':'coding_experience','q8': 'certificates', 'q11': 'experience_hackathone', 'q20': 'past_project','q30':'future_job','q31':'want_job'}, inplace=True)\n",
    "\n",
    "student_dataset = student_dataset.to_csv('student_dataset2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# student_dataset_2 = pd.read_csv('student_dataset2.csv')\n",
    "# display(student_dataset_2[['student_id','course_name', 'enrolled_course','coding_experience','certificates','experience_hackathone','past_project','future_job','want_job']])\n",
    "\n",
    "\n",
    "# print(\"Number of empty values rows\",student_dataset_2['enrolled_course'].isnull().sum())\n",
    "# display(student_dataset_2['course_name'].unique())\n",
    "# display(student_dataset_2['enrolled_course'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# selected_cols = student_dataset_2[['student_id','course_name', 'enrolled_course','coding_experience','certificates','experience_hackathone','past_project','future_job','want_job']]\n",
    "\n",
    "\n",
    "# combined_df = pd.concat([selected_cols, keywords_df], axis=1)\n",
    "# combined_df = combined_df[combined_df['course_name'] != '0']  \n",
    "# combined_df.to_csv('dummydata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_dataset = pd.read_csv('student_dataset2.csv')\n",
    "#conversion to lowercase.\n",
    "columns_to_lower = ['enrolled_course', 'coding_experience', 'experience_hackathone', 'past_project', 'future_job', 'certificates']\n",
    "# Replace empty values in 'future_job' column with 'None'\n",
    "student_dataset['future_job'].fillna('none', inplace=True)\n",
    "print(student_dataset.isnull().sum())\n",
    "student_dataset[columns_to_lower] = student_dataset[columns_to_lower].apply(lambda x: x.str.lower())\n",
    "\n",
    "enrolled_course_mapping = {\n",
    "    'cs': 'CSE',\n",
    "    'c.s': 'CSE',\n",
    "    'cse': 'CSE',\n",
    "    'cse ': 'CSE',\n",
    "    'computer science':'CSE',\n",
    "    'compuer science': 'CSE',\n",
    "    'computer science ':'CSE',\n",
    "    'c.s.e.': 'CSE',\n",
    "    'cse(da)': 'CSE',\n",
    "    'cse (da)':'CSE',\n",
    "    'cs(ccv)': 'CSE',\n",
    "    'ccv': 'CSE',\n",
    "    'c.s.e': 'CSE',\n",
    "    'da': 'CSE',\n",
    "    'cea': 'CSE',\n",
    "    'computer science and engineering': 'CSE',\n",
    "    'computer science and engineering ':'CSE',\n",
    "    'computer science engineering':'CSE',\n",
    "    'computer science engineering ': 'CSE',\n",
    "    'computer science & engineering':'CSE',\n",
    "    'computer science with specialization in cloud computing and virtualization': 'CSE',\n",
    "    'electronic and communication':'ECE',\n",
    "    'electronic and communication':'ECE',\n",
    "    'electronics and communication':'ECE',\n",
    "    'electronics and communication engineering':'ECE',\n",
    "    'electronics & communication engineering': 'ECE',\n",
    "    'electrical and electronics':'ECE',\n",
    "    'electrical': 'ECE',\n",
    "    'electrical engineering': 'ECE',\n",
    "    'electronics & communication': 'ECE',\n",
    "    'ece': 'ECE',\n",
    "    'ec': 'ECE',\n",
    "    'e.c.e': 'ECE',\n",
    "    'mechanical engineering': 'ME',\n",
    "    'mechanical enginering': 'ME',\n",
    "    'mechanical engineering.': 'ME',\n",
    "    'mechanical engineering ': 'ME',\n",
    "    'mechanical ':'ME',\n",
    "    'mechanical engg.': 'ME',\n",
    "    'civil engineering': 'CE',\n",
    "    'civil enginering': 'CE',\n",
    "    'civil': 'CE',\n",
    "    'ce': 'CE',\n",
    "    'me': 'ME',\n",
    "    'mechanical':'ME',\n",
    "    'mechanical':'ME',\n",
    "    'm.e.': 'ME',\n",
    "    'fb': 'FIN',\n",
    "    'fin':'FIN',\n",
    "    'marketing and finance': 'FIN',\n",
    "    'banking': 'FIN',\n",
    "    'biotechnology': 'BIOTECH',\n",
    "    'btech':'BIOTECH',\n",
    "    'biotech':'BIOTECH'\n",
    "}\n",
    "course_mapping = {\n",
    "    'B.com(H)':'B.Com(Hons)',\n",
    "    'B.com(hons.)':'B.Com(Hons)',\n",
    "    'B.com(h)':'B.Com(Hons)',\n",
    "    'Bcom(H)': 'B.Com(Hons)',\n",
    "    'Bcom hons.':'B.Com(Hons)',\n",
    "    'Bcom(hons.)':'B.Com(Hons)',\n",
    "    'B.com(Hons.)':'B.Com(Hons)',\n",
    "    'B.com(H) 2nd year':'B.Com(Hons)'\n",
    "}\n",
    "student_dataset['enrolled_course'] = student_dataset['enrolled_course'].map(enrolled_course_mapping).fillna(student_dataset['enrolled_course'])\n",
    "student_dataset['course_name'] = student_dataset['course_name'].map(course_mapping).fillna(student_dataset['course_name'])\n",
    "\n",
    "student_dataset = student_dataset[student_dataset['course_name']!= '0']\n",
    "student_dataset = student_dataset[['student_id','course_name','enrolled_course','coding_experience','certificates','experience_hackathone','past_project','future_job','want_job']]\n",
    "student_dataset = student_dataset.to_csv('temp_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "column value with '0'is eliminated and temp_dataset is created.\n",
    "course modules are defined with affilated student course.\n",
    "data is cleaned and normalized.\n",
    "Empty values are filled up with relavent course  according to student course and future jobs informations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "student_dataset = pd.read_csv('temp_dataset.csv')\n",
    "# handling empty values in course name\n",
    "for index, row in student_dataset.iterrows():\n",
    "    if pd.isna(row['enrolled_course']) and row['course_name']=='BCA':\n",
    "        student_dataset.at[index, 'enrolled_course'] = 'DS'\n",
    "    elif pd.isna(row['enrolled_course']) and row['course_name']=='B.Tech' and row['future_job'] in['software/ website developer','enterpreneur','income tax officer','data analyst/scientist','penetration and vulnerability tester']:\n",
    "        student_dataset.at[index, 'enrolled_course'] = 'CSE'\n",
    "    elif pd.isna(row['enrolled_course'])and row['course_name']=='B.Com(Hons)'and row['future_job'] in ['system/ database admin','business developer','a employee','no','accountant','i wanna be an officer in the indian armed forces']:\n",
    "        student_dataset.at[index, 'enrolled_course'] = 'Hons'\n",
    "    elif pd.isna(row['enrolled_course']) and row['certificates'] in ['java','r','c/c++'] and row['course_name']=='MBA':\n",
    "        student_dataset.at[index, 'enrolled_course'] = 'BA'\n",
    "    elif pd.isna(row['enrolled_course']) and row['course_name']=='MBA':\n",
    "        student_dataset.at[index, 'enrolled_course'] = 'MBA'\n",
    "    elif pd.isna(row['enrolled_course']) and row['course_name']=='MCA':\n",
    "        student_dataset.at[index, 'enrolled_course'] = 'MCA'\n",
    "    elif pd.isna(row['enrolled_course']) and row['course_name']=='BBA':\n",
    "        student_dataset.at[index, 'enrolled_course'] = 'MS'\n",
    "    elif row['enrolled_course']=='no' and row['course_name']=='B.Com(Hons)':\n",
    "        student_dataset.at[index, 'enrolled_course'] = 'B.Com(Hons)'\n",
    "    elif pd.isna(row['enrolled_course']) and row['course_name']=='B.Pharma':\n",
    "        student_dataset.at[index, 'enrolled_course'] = 'B.Pharma'\n",
    "    elif pd.isna(row['enrolled_course']) and row['course_name']=='B.Tech' and row['future_job'] in ['mechanical engg','']:\n",
    "        student_dataset.at[index, 'enrolled_course'] = 'ME'\n",
    "    # Add more conditions as needed\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "# student_dataset.drop(columns=['TF-IDF Score'], inplace= True)\n",
    "# student_dataset.drop(columns=['TF-IDF Score'], inplace= True)\n",
    "student_dataset.to_csv('cleaned_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Course moudels text extractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.gla.ac.in/courses/graduate/btech-computer-science-engineering\n",
      "https://www.gla.ac.in/courses/post-graduate/mba\n",
      "https://www.gla.ac.in/courses/diploma/computer-science-engineering\n",
      "https://www.gla.ac.in/courses/graduate/btech-electronics-communication-engineering\n",
      "https://www.gla.ac.in/courses/graduate/bba-management-science\n",
      "https://www.gla.ac.in/courses/graduate/bca-in-data-science\n",
      "https://www.gla.ac.in/courses/graduate/btech-mechanical-engineering\n",
      "https://www.gla.ac.in/courses/graduate/btech-civil-engineering\n",
      "https://www.gla.ac.in/courses/graduate/bsc-biotech-hons-by-research\n",
      "https://www.gla.ac.in/courses/graduate/bcom-hons-by-research\n",
      "https://www.gla.ac.in/courses/graduate/btech-biotechnology\n",
      "https://www.gla.ac.in/courses/graduate/bba-family-business\n",
      "https://www.gla.ac.in/courses/graduate/bcom-hons-by-research\n",
      "https://www.gla.ac.in/courses/post-graduate/mba-in-business-analytics\n",
      "https://www.gla.ac.in/courses/diploma/electronics-communication-engineering\n",
      "https://www.gla.ac.in/courses/post-graduate/mca\n",
      "https://www.gla.ac.in/courses/post-graduate/mba-financial-markets-banking\n",
      "https://www.gla.ac.in/courses/graduate/bpharm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "\n",
    "clean_dataset = pd.read_csv('cleaned_dataset.csv')\n",
    "temp_dataset = clean_dataset[['course_name','enrolled_course']].drop_duplicates()\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "\n",
    "def WebScrap(url):\n",
    "    print(url)\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "           \n",
    "            # Find all elements with class name 'details_course'\n",
    "            course_details = soup.find_all(class_='details-section')\n",
    "            carrer_info = soup.find_all(class_='detail-top-secs')\n",
    "            raw_text = ''\n",
    "            info_txt = ''\n",
    "            # Extract and print the text content of paragraphs inside 'details_course'\n",
    "            for detail in course_details:\n",
    "                paragraphs = detail.find_all('p')\n",
    "                for p in paragraphs:\n",
    "                    raw_text+=(p.text.strip())\n",
    "            for career_data in carrer_info:\n",
    "                data_info = career_data.find_all('ul')\n",
    "                for li in data_info:\n",
    "                    info_txt+=(li.text.strip())\n",
    "            return raw_text+info_txt\n",
    "            \n",
    "        else:\n",
    "            print(f\"Failed to retrieve data from {url}. Status code: {response.status_code}\")\n",
    "           \n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print(\"Inside\")\n",
    "\n",
    "########################################\n",
    "########################################\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for index, row in temp_dataset.iterrows():\n",
    "    column_val1 = row['course_name']\n",
    "    column_val2 = row['enrolled_course']\n",
    "\n",
    "    if column_val1 =='B.Tech' and column_val2=='CSE':\n",
    "        url = \"https://www.gla.ac.in/courses/graduate/btech-computer-science-engineering\"\n",
    "    elif column_val1=='B.Tech' and column_val2=='ECE':\n",
    "        url = \"https://www.gla.ac.in/courses/graduate/btech-electronics-communication-engineering\"\n",
    "       \n",
    "    elif column_val1=='B.Tech' and column_val2=='ME':\n",
    "        url = \"https://www.gla.ac.in/courses/graduate/btech-mechanical-engineering\"\n",
    "       \n",
    "    elif column_val1=='B.Tech' and column_val2=='CE':\n",
    "        url = \"https://www.gla.ac.in/courses/graduate/btech-civil-engineering\"\n",
    "        \n",
    "    elif column_val1=='B.Tech' and column_val2=='BIOTECH':\n",
    "        url = \"https://www.gla.ac.in/courses/graduate/btech-biotechnology\"\n",
    "\n",
    "    elif column_val1=='Bsc' and column_val2=='BIOTECH':\n",
    "        url = \"https://www.gla.ac.in/courses/graduate/bsc-biotech-hons-by-research\"\n",
    "       \n",
    "    elif column_val1=='MBA' and column_val2=='MBA':\n",
    "        url = \"https://www.gla.ac.in/courses/post-graduate/mba\"\n",
    "       \n",
    "    elif column_val1=='MBA' and column_val2=='BA':\n",
    "        url = \"https://www.gla.ac.in/courses/post-graduate/mba-in-business-analytics\"\n",
    "    elif column_val1=='MBA' and column_val2=='FIN':\n",
    "        url = \"https://www.gla.ac.in/courses/post-graduate/mba-financial-markets-banking\"\n",
    "       \n",
    "    elif column_val1 == 'MCA' and column_val2=='MCA':\n",
    "        url = \"https://www.gla.ac.in/courses/post-graduate/mca\"\n",
    "       \n",
    "    elif column_val1=='BBA' and column_val2=='MS':\n",
    "        url = \"https://www.gla.ac.in/courses/graduate/bba-management-science\"\n",
    "       \n",
    "    elif column_val1=='BBA' and column_val2=='FIN':\n",
    "        url = \"https://www.gla.ac.in/courses/graduate/bba-family-business\"\n",
    "       \n",
    "    elif column_val1=='B.Com(Hons)' and column_val2=='Hons':\n",
    "        url = \"https://www.gla.ac.in/courses/graduate/bcom-hons-by-research\"\n",
    "    elif column_val1=='B.Com(Hons)' and column_val2=='B.Com(Hons)':\n",
    "        url = \"https://www.gla.ac.in/courses/graduate/bcom-hons-by-research\"\n",
    "       \n",
    "    elif column_val1=='BCA' and column_val2=='DS':\n",
    "        url = \"https://www.gla.ac.in/courses/graduate/bca-in-data-science\"\n",
    "    elif column_val1=='B.Pharma' and column_val2=='B.Pharma':\n",
    "        url = \"https://www.gla.ac.in/courses/graduate/bpharm\"\n",
    "    elif column_val1=='Polytechnic' and column_val2=='CE':\n",
    "        url = \"https://www.gla.ac.in/courses/diploma/electronics-communication-engineering\"\n",
    "    elif column_val1=='Polytechnic' and column_val2=='CSE':\n",
    "        url = \"https://www.gla.ac.in/courses/diploma/computer-science-engineering\"\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    clean_Data = WebScrap(url)\n",
    "    mask = (clean_dataset['course_name'] == column_val1) & (clean_dataset['enrolled_course'] == column_val2)\n",
    "    clean_dataset.loc[mask, 'course_desc'] = clean_Data\n",
    "\n",
    "    \n",
    "clean_dataset.to_csv('cleaned_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "allowed_certificates =['c++','java','android','autocad','i\\'m accountant not a programmer ','cyber security','javascript','java , python','android, web development','','r','python','c#','solidworks','c/c++','php','also in r from data camp, python from lynda','autocad,staadpro','html,css,javascript,php']\n",
    "clean_dataset['certificates'] = clean_dataset['certificates'].apply(lambda x: 'none' if x not in allowed_certificates else x)\n",
    "\n",
    "coding_experience = ['yes','no']\n",
    "clean_dataset['coding_experience'] = clean_dataset['coding_experience'].apply(lambda x: 'no'if x not in coding_experience else x)\n",
    "\n",
    "hackthon_experience = ['yes','no']\n",
    "clean_dataset['experience_hackathone'] = clean_dataset['experience_hackathone'].apply(lambda x: 'no'if x not in coding_experience else x)\n",
    "\n",
    "\n",
    "clean_dataset.to_csv('cleaned_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion to binary form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "import pandas as pd\n",
    "clean_dataset = pd.read_csv('cleaned_dataset.csv')\n",
    "clean_dataset['coding_experience'] = clean_dataset['coding_experience'].apply(lambda x: 1 if x == 'yes' else 0)\n",
    "clean_dataset['experience_hackathone'] = clean_dataset['experience_hackathone'].apply(lambda x: 1 if x == 'yes' else 0)\n",
    "clean_dataset['past_project'] = clean_dataset['past_project'].apply(lambda x: 1 if x == 'yes' else 0)\n",
    "clean_dataset.to_csv('cleaned_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "\n",
    "clean_dataset = pd.read_csv('cleaned_dataset.csv')\n",
    "text_columns = ['course_desc']\n",
    "print(clean_dataset.isnull().sum())\n",
    "\n",
    "def dataClean(raw_data):\n",
    "    cleaned_student_profile= raw_data.lower()\n",
    "\n",
    "    cleaned_student_profile = cleaned_student_profile.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenization\n",
    "    \n",
    "    text_tokens = word_tokenize(cleaned_student_profile)\n",
    "    # Removing stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text_tokens = [word for word in text_tokens if word not in stop_words]\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    text_tokens = [stemmer.stem(word) for word in text_tokens]\n",
    "    # Join tokens back into a string\n",
    "    cleaned_student_profile= ' '.join(text_tokens) \n",
    "      # Use TF-IDF to extract important keywords\n",
    "    tfidf = TfidfVectorizer(max_features=5)\n",
    "    tfidf.fit_transform([cleaned_student_profile])\n",
    "    keywords = tfidf.get_feature_names_out()\n",
    "    return keywords\n",
    "\n",
    "for col in text_columns:\n",
    "    clean_dataset[col] = clean_dataset[col].apply(dataClean)\n",
    "# Combine all text data for student dataset\n",
    "\n",
    "def course_clean(raw_data):\n",
    "    cleaned_text = raw_data.lower()\n",
    "    cleaned_text = re.sub(r'<.*?>', '', cleaned_text)\n",
    "    cleaned_text = cleaned_text.replace(',',\"\")\n",
    "    cleaned_text = re.sub(r'[^\\w\\s/()]', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text) \n",
    "    cleaned_text = re.sub(r'\\([^)]*\\)', '', cleaned_text)\n",
    "    # Join tokens back into a single string\n",
    "    # processed_text = ' '.join(tokens)\n",
    "    return cleaned_text\n",
    "course_column = ['course_name','enrolled_course']\n",
    "\n",
    "for col in course_column:\n",
    "    clean_dataset[col] = clean_dataset[col].apply(course_clean)\n",
    "\n",
    "\n",
    "student_process_Data = clean_dataset[['student_id','course_name','enrolled_course','future_job','coding_experience','certificates','experience_hackathone','past_project','course_desc']]\n",
    "student_process_Data.to_csv('StudentClean_dataset.csv')\n",
    "\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "job_dataset = pd.read_csv('job_postings.csv')\n",
    "job_dataset_col = ['title','description']\n",
    "non_empty_jobdata = job_dataset.dropna()\n",
    "\n",
    "\n",
    "def job_datasetClean(raw_jobData):\n",
    "    \n",
    "    cleaned_jobdata = raw_jobData.lower()\n",
    "\n",
    "    cleaned_jobdata = cleaned_jobdata.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenization\n",
    "    \n",
    "    job_tokens = word_tokenize(cleaned_jobdata)\n",
    "    # Removing stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    job_tokens = [word for word in job_tokens if word not in stop_words]\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    job_tokens = [stemmer.stem(word) for word in job_tokens]\n",
    "    # Join tokens back into a string\n",
    "    cleaned_jobdata = ' '.join(job_tokens)\n",
    "\n",
    "      # Use TF-IDF to extract important keywords\n",
    "    tfidf = TfidfVectorizer(max_features=5)\n",
    "    tfidf.fit_transform([cleaned_jobdata])\n",
    "    keywords = tfidf.get_feature_names_out()\n",
    "    return keywords\n",
    "\n",
    "job_dataset['description'] = job_dataset['description'].astype(str)\n",
    "job_dataset = job_dataset[['title','description','job_posting_url']]\n",
    "jobpost_columns = ['title','description']\n",
    "\n",
    "job_dataset['pro_title'] = job_dataset['title'].apply(course_clean)\n",
    "job_dataset['pro_description'] = job_dataset['description'].apply(job_datasetClean)\n",
    "job_dataset.to_csv('cleaned_jobdataset.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/bikash/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(253, 689)\n",
      "(33246, 273185)\n"
     ]
    }
   ],
   "source": [
    "# Combine all text data for student dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "\n",
    "Student_InfoDataset = pd.read_csv('StudentClean_dataset.csv')\n",
    "Job_InfoDataset = pd.read_csv('cleaned_jobdataset.csv')\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "text_columnData = ['course_name','enrolled_course','future_job','certificates','course_desc']\n",
    "job_text_columns = ['pro_title', 'pro_description']\n",
    "# Replace NaN values with an empty string in the selected columns\n",
    "Student_InfoDataset[text_columnData] = Student_InfoDataset[text_columnData].fillna('')\n",
    "Job_InfoDataset['pro_description'] = Job_InfoDataset['pro_description'].fillna('')\n",
    "# Combine all text data for student dataset\n",
    "combined_student_text = Student_InfoDataset[text_columnData].apply(lambda x: ' '.join(x), axis=1)\n",
    "\n",
    "# TF-IDF vectorization for combined student text\n",
    "tfidf_vectorizer_student = TfidfVectorizer(stop_words='english')\n",
    "student_tfidf = tfidf_vectorizer_student.fit_transform(combined_student_text)\n",
    "\n",
    "# Combine TF-IDF vectors with categorical variables for student profiles\n",
    "student_profile_data = hstack((student_tfidf, StandardScaler().fit_transform(Student_InfoDataset[['coding_experience', 'experience_hackathone','past_project']])))\n",
    "\n",
    "tfidf_vectorizer_job = TfidfVectorizer(stop_words='english')\n",
    "job_tfidf = tfidf_vectorizer_job.fit_transform(Job_InfoDataset[job_text_columns].apply(' '.join, axis=1))\n",
    "\n",
    "\n",
    "print(student_profile_data.shape)\n",
    "print(job_tfidf.shape)\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# student_train, student_test = train_test_split(student_profile_data, test_size=0.2, random_state=42)\n",
    "# job_train, job_test = train_test_split(job_tfidf, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# # Calculate cosine similarity between student profiles and job TF-IDF vectors for training set\n",
    "# similarity_matrix_train = cosine_similarity(student_train, job_train)\n",
    "\n",
    "# # Calculate cosine similarity between student profiles and job TF-IDF vectors for testing set\n",
    "# similarity_matrix_test = cosine_similarity(student_test, job_test)\n",
    "\n",
    "# # Function to recommend jobs for each student\n",
    "# def recommend_jobs(similarity_matrix, top_n=4):\n",
    "#     recommended_jobs = []\n",
    "#     for i in range(similarity_matrix.shape[0]):  # Iterate over each student\n",
    "#         top_jobs_indices = similarity_matrix[i].argsort()[-top_n:][::-1]  # Get indices of top N similar jobs\n",
    "#         top_jobs = Job_InfoDataset.iloc[top_jobs_indices]['job_title'].values.tolist()  # Get job titles\n",
    "#         recommended_jobs.append(top_jobs)\n",
    "#     return recommended_jobs\n",
    "\n",
    "# # Recommend jobs for students in the training set\n",
    "# recommended_jobs_train = recommend_jobs(similarity_matrix_train)\n",
    "\n",
    "# # Recommend jobs for students in the testing set\n",
    "# recommended_jobs_test = recommend_jobs(similarity_matrix_test)\n",
    "\n",
    "# # Print recommended jobs for the first student in the training set\n",
    "# print(\"Recommended Jobs for the First Student in the Training Set:\")\n",
    "# for job_title in recommended_jobs_train[0]:\n",
    "#     print(job_title)\n",
    "\n",
    "# # Print recommended jobs for the first student in the testing set\n",
    "# print(\"\\nRecommended Jobs for the First Student in the Testing Set:\")\n",
    "# for job_title in recommended_jobs_test[0]:\n",
    "#     print(job_title)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonboxenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
